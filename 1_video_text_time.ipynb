{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio extracted successfully.\n",
      "Preprocessing audio...\n",
      "Audio preprocessing completed.\n",
      "Loading Whisper model...\n",
      "Transcribing audio with Whisper...\n",
      "WARNING:tensorflow:From c:\\Users\\Akash\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The model 'XLMRobertaForTokenClassification' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error restoring punctuation: 'Text2TextGenerationPipeline' object has no attribute 'prefix'\n",
      "SRT file generated: output_subtitles_advanced.srt\n",
      "Final Transcription with Punctuation:\n",
      "\n",
      " You work hard, you make money, you do it for yourself. That's not life. You go out, you seek for people who need your help, you make their lives better. You become that sponge which can absorb all the negativity and you become that person who can emit beautiful positive vibes and when you realize that you have changed someone's life and because of you, this person didn't give up. That is the day when you live.\n"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "import whisper\n",
    "import srt\n",
    "import os\n",
    "import datetime\n",
    "import cv2\n",
    "import numpy as np\n",
    "import io\n",
    "from pydub import AudioSegment\n",
    "from transformers import pipeline\n",
    "import textwrap\n",
    "\n",
    "\n",
    "def extract_audio(video_path, output_audio_path):\n",
    "    \"\"\"Extract audio from video using FFmpeg.\"\"\"\n",
    "    try:\n",
    "        ffmpeg.input(video_path).output(output_audio_path).run(overwrite_output=True)\n",
    "        print(\"Audio extracted successfully.\")\n",
    "    except ffmpeg.Error as e:\n",
    "        print(f\"FFmpeg error: {e.stderr.decode()}\")\n",
    "\n",
    "def preprocess_audio(input_file):\n",
    "    \"\"\"\n",
    "    Preprocess the audio file:\n",
    "    - Converts stereo to mono.\n",
    "    - Resamples to 16kHz for compatibility with Whisper.\n",
    "    - Returns the processed audio as a NumPy array and sample rate.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Preprocessing audio...\")\n",
    "        audio = AudioSegment.from_file(input_file)\n",
    "        audio = audio.set_channels(1)  # Convert to mono\n",
    "        audio = audio.set_frame_rate(16000)  # Resample to 16kHz\n",
    "        \n",
    "        # Export to raw audio bytes\n",
    "        raw_audio = io.BytesIO()\n",
    "        audio.export(raw_audio, format=\"wav\")\n",
    "        raw_audio.seek(0)\n",
    "\n",
    "        # Load audio into a NumPy array\n",
    "        waveform = np.frombuffer(raw_audio.read(), dtype=np.int16)\n",
    "        sample_rate = 16000  # Since we resampled to 16kHz\n",
    "        print(\"Audio preprocessing completed.\")\n",
    "        return waveform, sample_rate\n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def transcribe_with_whisper(waveform, sample_rate):\n",
    "    \"\"\"\n",
    "    Transcribe the given audio waveform to text using Whisper.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Loading Whisper model...\")\n",
    "        model = whisper.load_model(\"base\")  # large\n",
    "        print(\"Transcribing audio with Whisper...\")\n",
    "        \n",
    "        # Whisper requires audio in float32 format, normalized to [-1, 1]\n",
    "        audio_float = waveform.astype(np.float32) / 32768.0\n",
    "        \n",
    "        # Transcribe with Whisper\n",
    "        result = model.transcribe(audio_float, fp16=False)\n",
    "        return result[\"text\"], result[\"segments\"]\n",
    "    except Exception as e:\n",
    "        return f\"Error during transcription: {e}\", None\n",
    "\n",
    "def generate_srt_from_segments(segments, output_srt_path):\n",
    "    \"\"\"Generate SRT file from Whisper segments.\"\"\"\n",
    "    try:\n",
    "        subtitles = []\n",
    "        for i, segment in enumerate(segments):\n",
    "            start_time = datetime.timedelta(seconds=segment[\"start\"])\n",
    "            end_time = datetime.timedelta(seconds=segment[\"end\"])\n",
    "            subtitle = srt.Subtitle(index=i + 1, start=start_time, end=end_time, content=segment[\"text\"])\n",
    "            subtitles.append(subtitle)\n",
    "        with open(output_srt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(srt.compose(subtitles))\n",
    "        print(f\"SRT file generated: {output_srt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating SRT file: {e}\")\n",
    "\n",
    "def restore_punctuation(text):\n",
    "    \"\"\"Restore punctuation using a Hugging Face model.\"\"\"\n",
    "    try:\n",
    "        punctuator = pipeline(\"text2text-generation\", model=\"oliverguhr/fullstop-punctuation-multilang-large\")\n",
    "        restored_text = punctuator(text, max_length=1024)[0][\"generated_text\"]\n",
    "        print(\"Punctuation restored successfully.\")\n",
    "        return restored_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error restoring punctuation: {e}\")\n",
    "        return text\n",
    "\n",
    "def video_to_text_advanced(video_path, output_srt_path):\n",
    "    \"\"\"Main function to process video and generate accurate subtitles.\"\"\"\n",
    "    raw_audio_path = \"temp_raw_audio.wav\"\n",
    "    try:\n",
    "        # Extract audio from video\n",
    "        extract_audio(video_path, raw_audio_path)\n",
    "\n",
    "        # Preprocess audio for Whisper\n",
    "        waveform, sample_rate = preprocess_audio(raw_audio_path)\n",
    "        if waveform is None:\n",
    "            print(\"Audio preprocessing failed.\")\n",
    "            return\n",
    "\n",
    "        # Transcribe audio with Whisper\n",
    "        transcript, segments = transcribe_with_whisper(waveform, sample_rate)\n",
    "        if segments is None:\n",
    "            print(\"Transcription failed.\")\n",
    "            return\n",
    "\n",
    "        # Restore punctuation\n",
    "        transcript_with_punctuation = restore_punctuation(transcript)\n",
    "\n",
    "        # Generate SRT file\n",
    "        generate_srt_from_segments(segments, output_srt_path)\n",
    "\n",
    "        # Display the final transcription\n",
    "        print(\"Final Transcription with Punctuation:\\n\")\n",
    "        print(transcript_with_punctuation)\n",
    "    finally:\n",
    "        if os.path.exists(raw_audio_path):\n",
    "            os.remove(raw_audio_path)\n",
    "\n",
    "def wrap_text(text, width=50):\n",
    "    \"\"\"Wrap text into lines of specified width.\"\"\"\n",
    "    return \"\\n\".join(textwrap.wrap(text, width))\n",
    "\n",
    "def synchronize_text_with_video(video_path, srt_path):\n",
    "    \"\"\"Display subtitles on video.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file.\")\n",
    "        return\n",
    "    \n",
    "    with open(srt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        subtitles = list(srt.parse(f.read()))\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 1\n",
    "    font_color = (255, 255, 255)\n",
    "    thickness = 2\n",
    "    \n",
    "    frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    subtitle_index = 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        current_time = datetime.timedelta(seconds=cap.get(cv2.CAP_PROP_POS_MSEC) / 1000)\n",
    "        if subtitle_index < len(subtitles):\n",
    "            subtitle = subtitles[subtitle_index]\n",
    "            if subtitle.start <= current_time <= subtitle.end:\n",
    "                cv2.putText(frame, subtitle.content, (50, 50), font, font_scale, font_color, thickness)\n",
    "            elif current_time > subtitle.end:\n",
    "                subtitle_index += 1\n",
    "        \n",
    "        frame = cv2.resize(frame, (500, 750))\n",
    "        cv2.imshow(\"Video with Subtitles\", frame)\n",
    "        if cv2.waitKey(1000 // frame_rate) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "video_path = \"best_advice_for_life.mp4\"\n",
    "output_srt_path = \"output_subtitles_advanced.srt\"\n",
    "\n",
    "# Process video and generate subtitles\n",
    "video_to_text_advanced(video_path, output_srt_path)\n",
    "\n",
    "synchronize_text_with_video(video_path, output_srt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
