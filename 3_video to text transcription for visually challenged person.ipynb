{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio extracted successfully.\n",
      "Preprocessing audio...\n",
      "Audio preprocessing completed.\n",
      "Loading Whisper model...\n",
      "Transcribing audio with Whisper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The model 'XLMRobertaForTokenClassification' is not supported for text2text-generation. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'Qwen2AudioForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error restoring punctuation: 'Text2TextGenerationPipeline' object has no attribute 'prefix'\n",
      "SRT file generated: output_subtitles_advanced.srt\n",
      "Final Transcription with Punctuation:\n",
      "\n",
      " Ever tried, ever failed. No matter. Try again. Fail again. Fail better.\n",
      "Conversion complete! Braille text saved to: braille_output.txt\n"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "import whisper\n",
    "import srt\n",
    "import os\n",
    "import datetime\n",
    "import cv2\n",
    "import numpy as np\n",
    "import io\n",
    "from pydub import AudioSegment\n",
    "from transformers import pipeline\n",
    "import textwrap\n",
    "import re\n",
    "\n",
    "\n",
    "def extract_audio(video_path, output_audio_path):\n",
    "    \"\"\"Extract audio from video using FFmpeg.\"\"\"\n",
    "    try:\n",
    "        ffmpeg.input(video_path).output(output_audio_path).run(overwrite_output=True)\n",
    "        print(\"Audio extracted successfully.\")\n",
    "    except ffmpeg.Error as e:\n",
    "        print(f\"FFmpeg error: {e.stderr.decode()}\")\n",
    "\n",
    "def preprocess_audio(input_file):\n",
    "    \"\"\"\n",
    "    Preprocess the audio file:\n",
    "    - Converts stereo to mono.\n",
    "    - Resamples to 16kHz for compatibility with Whisper.\n",
    "    - Returns the processed audio as a NumPy array and sample rate.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Preprocessing audio...\")\n",
    "        audio = AudioSegment.from_file(input_file)\n",
    "        audio = audio.set_channels(1)  # Convert to mono\n",
    "        audio = audio.set_frame_rate(16000)  # Resample to 16kHz\n",
    "        \n",
    "        # Export to raw audio bytes\n",
    "        raw_audio = io.BytesIO()\n",
    "        audio.export(raw_audio, format=\"wav\")\n",
    "        raw_audio.seek(0)\n",
    "\n",
    "        # Load audio into a NumPy array\n",
    "        waveform = np.frombuffer(raw_audio.read(), dtype=np.int16)\n",
    "        sample_rate = 16000  # Since we resampled to 16kHz\n",
    "        print(\"Audio preprocessing completed.\")\n",
    "        return waveform, sample_rate\n",
    "    except Exception as e:\n",
    "        print(f\"Error during preprocessing: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def transcribe_with_whisper(waveform, sample_rate):\n",
    "    \"\"\"\n",
    "    Transcribe the given audio waveform to text using Whisper.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Loading Whisper model...\")\n",
    "        model = whisper.load_model(\"base\")  # large\n",
    "        print(\"Transcribing audio with Whisper...\")\n",
    "        \n",
    "        # Whisper requires audio in float32 format, normalized to [-1, 1]\n",
    "        audio_float = waveform.astype(np.float32) / 32768.0\n",
    "        \n",
    "        # Transcribe with Whisper\n",
    "        result = model.transcribe(audio_float, fp16=False)\n",
    "        return result[\"text\"], result[\"segments\"]\n",
    "    except Exception as e:\n",
    "        return f\"Error during transcription: {e}\", None\n",
    "\n",
    "def generate_srt_from_segments(segments, output_srt_path):\n",
    "    \"\"\"Generate SRT file from Whisper segments.\"\"\"\n",
    "    try:\n",
    "        subtitles = []\n",
    "        for i, segment in enumerate(segments):\n",
    "            start_time = datetime.timedelta(seconds=segment[\"start\"])\n",
    "            end_time = datetime.timedelta(seconds=segment[\"end\"])\n",
    "            subtitle = srt.Subtitle(index=i + 1, start=start_time, end=end_time, content=segment[\"text\"])\n",
    "            subtitles.append(subtitle)\n",
    "        with open(output_srt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(srt.compose(subtitles))\n",
    "        print(f\"SRT file generated: {output_srt_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating SRT file: {e}\")\n",
    "\n",
    "def restore_punctuation(text):\n",
    "    \"\"\"Restore punctuation using a Hugging Face model.\"\"\"\n",
    "    try:\n",
    "        punctuator = pipeline(\"text2text-generation\", model=\"oliverguhr/fullstop-punctuation-multilang-large\")\n",
    "        restored_text = punctuator(text, max_length=1024)[0][\"generated_text\"]\n",
    "        print(\"Punctuation restored successfully.\")\n",
    "        return restored_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error restoring punctuation: {e}\")\n",
    "        return text\n",
    "\n",
    "def video_to_text_advanced(video_path, output_srt_path):\n",
    "    \"\"\"Main function to process video and generate accurate subtitles.\"\"\"\n",
    "    raw_audio_path = \"temp_raw_audio.wav\"\n",
    "    try:\n",
    "        # Extract audio from video\n",
    "        extract_audio(video_path, raw_audio_path)\n",
    "\n",
    "        # Preprocess audio for Whisper\n",
    "        waveform, sample_rate = preprocess_audio(raw_audio_path)\n",
    "        if waveform is None:\n",
    "            print(\"Audio preprocessing failed.\")\n",
    "            return\n",
    "\n",
    "        # Transcribe audio with Whisper\n",
    "        transcript, segments = transcribe_with_whisper(waveform, sample_rate)\n",
    "        if segments is None:\n",
    "            print(\"Transcription failed.\")\n",
    "            return\n",
    "\n",
    "        # Restore punctuation\n",
    "        transcript_with_punctuation = restore_punctuation(transcript)\n",
    "\n",
    "        # Generate SRT file\n",
    "        generate_srt_from_segments(segments, output_srt_path)\n",
    "\n",
    "        # Display the final transcription\n",
    "        print(\"Final Transcription with Punctuation:\\n\")\n",
    "        print(transcript_with_punctuation)\n",
    "    finally:\n",
    "        if os.path.exists(raw_audio_path):\n",
    "            os.remove(raw_audio_path)\n",
    "\n",
    "def wrap_text(text, width=50):\n",
    "    \"\"\"Wrap text into lines of specified width.\"\"\"\n",
    "    return \"\\n\".join(textwrap.wrap(text, width))\n",
    "\n",
    "# Braille Unicode character mapping\n",
    "braille_alphabet = {\n",
    "    'a': '⠁', 'b': '⠃', 'c': '⠉', 'd': '⠙', 'e': '⠑', 'f': '⠋', 'g': '⠛', 'h': '⠓', \n",
    "    'i': '⠊', 'j': '⠚', 'k': '⠅', 'l': '⠇', 'm': '⠍', 'n': '⠝', 'o': '⠕', 'p': '⠏', \n",
    "    'q': '⠟', 'r': '⠗', 's': '⠎', 't': '⠞', 'u': '⠥', 'v': '⠧', 'w': '⠺', 'x': '⠭', \n",
    "    'y': '⠽', 'z': '⠵', '1': '⠁', '2': '⠃', '3': '⠉', '4': '⠙', '5': '⠑', '6': '⠋', \n",
    "    '7': '⠛', '8': '⠓', '9': '⠊', '0': '⠚', ' ': ' ', ',': '⠂', '.': '⠲', '?': '⠦', \n",
    "    '!': '⠮', ':': '⠰', ';': '⠱', '-': '⠤', '\"': '⠦', \"'\": '⠄'\n",
    "}\n",
    "\n",
    "def convert_to_braille(text):\n",
    "    \"\"\"\n",
    "    Converts the given text to Braille using Unicode Braille patterns.\n",
    "    \"\"\"\n",
    "    braille_text = ''\n",
    "    for char in text.lower():\n",
    "        braille_text += braille_alphabet.get(char, '')  # Get Braille character or skip if not found\n",
    "    return braille_text\n",
    "\n",
    "def process_srt_file(file_path):\n",
    "    \"\"\"\n",
    "    Processes the SRT file, extracts the text and converts it to Braille.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Regular expression to extract subtitle text from the SRT file\n",
    "    subtitle_blocks = re.findall(r'\\d+\\n(.*?)(?=\\n\\d+|$)', content, re.DOTALL)\n",
    "    \n",
    "    braille_subtitles = []\n",
    "    \n",
    "    for block in subtitle_blocks:\n",
    "        # Remove timestamps and extra whitespace\n",
    "        text = re.sub(r'\\d{2}:\\d{2}:\\d{2},\\d{3}', '', block).strip()\n",
    "        braille_text = convert_to_braille(text)\n",
    "        braille_subtitles.append(braille_text)\n",
    "    \n",
    "    return '\\n'.join(braille_subtitles)\n",
    "\n",
    "def save_braille_output(output_file, braille_text):\n",
    "    \"\"\"\n",
    "    Saves the Braille text to an output file.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write(braille_text)\n",
    "\n",
    "\n",
    "def wrap_text_to_width(text, max_pixel_width, font, scale, thickness):\n",
    "    \"\"\"Wrap text into multiple lines so each line fits within max_pixel_width.\"\"\"\n",
    "    words = text.split()\n",
    "    lines = []\n",
    "    current_line = \"\"\n",
    "\n",
    "    for word in words:\n",
    "        test_line = current_line + \" \" + word if current_line else word\n",
    "        text_size = cv2.getTextSize(test_line, font, scale, thickness)[0][0]\n",
    "        if text_size <= max_pixel_width:\n",
    "            current_line = test_line\n",
    "        else:\n",
    "            lines.append(current_line)\n",
    "            current_line = word\n",
    "\n",
    "    if current_line:\n",
    "        lines.append(current_line)\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "def synchronize_text_with_video(video_path, srt_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file.\")\n",
    "        return\n",
    "\n",
    "    # Load subtitles\n",
    "    with open(srt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        subtitles = list(srt.parse(f.read()))\n",
    "\n",
    "    # Video properties\n",
    "    frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Font settings (scaled based on video height)\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = height / 720 * 0.8\n",
    "    thickness = 2\n",
    "    color = (255, 255, 255)\n",
    "    margin = 30  # left/right margin from edge\n",
    "\n",
    "    subtitle_index = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        width = 400\n",
    "        height = 600\n",
    "        frame = cv2.resize(frame, (width, height))\n",
    "\n",
    "        current_time = datetime.timedelta(seconds=cap.get(cv2.CAP_PROP_POS_MSEC) / 1000)\n",
    "        if subtitle_index < len(subtitles):\n",
    "            sub = subtitles[subtitle_index]\n",
    "            if sub.start <= current_time <= sub.end:\n",
    "                lines = wrap_text_to_width(sub.content, width - 2 * margin, font, font_scale, thickness)\n",
    "\n",
    "                for i, line in enumerate(lines):\n",
    "                    text_size = cv2.getTextSize(line, font, font_scale, thickness)[0]\n",
    "                    x = (width - text_size[0]) // 2\n",
    "                    y = height - 50 - (len(lines) - i - 1) * int(40 * font_scale)\n",
    "                    # Draw background box\n",
    "                    (text_w, text_h), baseline = cv2.getTextSize(line, font, font_scale, thickness)\n",
    "                    box_coords = ((x - 10, y - text_h - 10), (x + text_w + 10, y + baseline + 10))\n",
    "\n",
    "                    overlay = frame.copy()\n",
    "                    cv2.rectangle(overlay, box_coords[0], box_coords[1], (0, 0, 0), cv2.FILLED)\n",
    "                    alpha = 0.5\n",
    "                    cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0, frame)\n",
    "\n",
    "                    # Draw text\n",
    "                    cv2.putText(frame, line, (x, y), font, font_scale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "            elif current_time > sub.end:\n",
    "                subtitle_index += 1\n",
    "\n",
    "        cv2.imshow(\"Video with Subtitles\", frame)\n",
    "        if cv2.waitKey(1000 // frame_rate) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "# video_path = \"best_advice_for_life.mp4\"\n",
    "video_path = \"motivation.mp4\"\n",
    "output_srt_path = \"output_subtitles_advanced.srt\"\n",
    "\n",
    "# Example usage\n",
    "input_srt_file = output_srt_path  # Path to your SRT file\n",
    "output_braille_file = 'braille_output.txt'  # Output file to store Braille text\n",
    "\n",
    "\n",
    "# Process video and generate subtitles\n",
    "video_to_text_advanced(video_path, output_srt_path)\n",
    "\n",
    "synchronize_text_with_video(video_path, output_srt_path)\n",
    "\n",
    "braille_text = process_srt_file(input_srt_file)\n",
    "save_braille_output(output_braille_file, braille_text)\n",
    "\n",
    "print(\"Conversion complete! Braille text saved to:\", output_braille_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\whisper\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "print(model.device)  # Check if it's running on CPU or GPU\n",
    "print(whisper.__file__)  # Get the location of the whisper module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
